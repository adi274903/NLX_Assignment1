**Phase 3 - Evaluation Data**
----


In the given phase, the ChatGPT 4o model and Gemini 2.5 pro model's were run in their respective browser based format. The MedGemma 4b was run on a colab instance and the code for the same would be made available in the medgemma.ipynb file for running and analysis of the given model.

Prompts are available in the Phase_2_Answers.md file and the output given by each model would be made available as a link to the web site or through the ipynb respectively. A proper documentation is further available in the Phase_3_Results.xlsx file.

For evaluation, the given steps/ procedures were undertaken:

1. The Task and prompt of each is opened in a new chat window to ensure no context is available for each of the AI model. This is to ensure an unbiased response to measure its domain appropriateness each time.
2. The response is recorded as it is and no alteration is made for the same. The responses will be available as a link to the chat or as screenshots to the model's output.
3. If 100% of the prompt is accurate, the model would receive the 5 score on the Likert Scale. If the model is partially correct or the model's reasoning is flawed, the model would receive 3 and if it's logic and answer is wrong, the model would receive 0 on the scale.
4. The following models are assessed on their accuracy, relevance and domain appropriatness. Remarks are also given of why the model was scored as such.

 The following results can be viewed in Phase_3_Results.xlsx file
